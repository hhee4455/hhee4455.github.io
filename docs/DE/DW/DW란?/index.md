---
layout: default
title: 데이터 웨어하우스란?
nav_order: 1
has_children: false
permalink: /docs/DE/DW/DW란?
parent: DW
grand_parent: 데이터 엔지니어
---
# 다양한 데이터 웨어하우스 옵션

## 데이터 팀의 역할
- 데이터 팀의 일반적인 비전
- 데이터 팀의 발전 단계
- 데이터 팀의 역할과 회사 성장

### 데이터 조직의 비전
- 신뢰할 수 있는 데이터를 바탕으로 부가 가치 생성
- Data is the new oil

### 데이터 조직이 하는 일
1. 고품질 데이터를 기반으로 의사 결정권자에게 입력 제공
   - 데이터 기반 지표 정의, 대시보드와 리포트 생성
2. 고품질 데이터를 기반으로 사용자 서비스 경험 개선
   - 머신 러닝을 통한 서비스 경험 개선

## 데이터 팀의 발전 단계
1. 데이터 인프라 구축
   - ETL, 데이터 웨어하우스 구축
2. 데이터 분석 수행
   - 데이터 분석, 지표 정의, 시각화, 리포팅
3. 데이터 과학 적용
   - 머신러닝 모델을 통한 사용자 경험 개선

## 데이터 조직의 구성원
### 데이터 엔지니어

#### 역할
- 데이터 웨어하우스와 ETL 구축
- 데이터 분석가, 데이터 과학자와 협업하여 필요한 툴과 데이터를 제공

#### 데이터 엔지니어가 알아야 하는 기술
1. **SQL (Structured Query Language)**
   - 기본 SQL: 데이터베이스와 상호작용하기 위한 기본 쿼리 언어
   - Hive, Presto, SparkSQL: 빅데이터 환경에서 사용하는 SQL 변형

2. **프로그래밍 언어**
   - **Python**: 데이터 처리, 자동화, ETL 작업에 널리 사용됨
   - **Java**: 대규모 데이터 처리 및 백엔드 시스템 구축에 사용
   - **Scala**: Apache Spark와 같은 빅데이터 처리 시스템에서 사용

3. **데이터 웨어하우스**
   - **AWS Redshift**: 대규모 데이터 처리와 분석을 위한 클라우드 데이터 웨어하우스
   - **Snowflake**: 클라우드 기반 데이터 웨어하우스, 데이터 공유 및 마켓플레이스 제공
   - **Google BigQuery**: 구글 클라우드의 서버리스 데이터 웨어하우스

4. **ETL/ELT 프레임워크**
   - **Airflow**: 파이썬 기반의 워크플로우 관리 툴로, ETL 작업의 스케줄링과 모니터링에 사용
   - **dbt (data build tool)**: ELT 작업을 위한 분석 엔지니어링 툴

5. **대용량 데이터 처리 플랫폼**
   - **Apache Spark**: 배치 및 실시간 데이터 처리를 위한 분산 데이터 처리 시스템
   - **Hadoop YARN**: 클러스터 리소스를 관리하고 스케줄링하는 시스템

6. **컨테이너 기술**
   - **Docker**: 애플리케이션의 배포와 관리를 위한 컨테이너화 기술
   - **Kubernetes (K8s)**: 컨테이너화된 애플리케이션의 자동 배포, 스케일링, 관리를 위한 오픈소스 시스템

7. **클라우드 컴퓨팅**
   - **AWS (Amazon Web Services)**: 클라우드 서비스 제공 플랫폼
   - **GCP (Google Cloud Platform)**: 구글의 클라우드 컴퓨팅 서비스
   - **Azure**: 마이크로소프트의 클라우드 서비스

8. **기타 유용한 지식**
   - **머신러닝 일반**: 데이터 과학과 머신러닝 모델에 대한 기본 이해
   - **A/B 테스트와 통계**: 실험 설계와 데이터 분석을 위한 통계적 방법론

![Data Engineering Roadmap](/assets/images/roadmap.png)

### 데이터 분석가
- 비즈니스 인텔리전스 책임
- 대시보드 생성 (Tableau, Looker, Superset)
- 비즈니스 도메인에 대한 깊은 지식 필요

### 데이터 과학자
- 머신러닝 모델 구축 및 적용
- 데이터 기반 미래 예측 및 서비스 개선

### 새로운 직군들
- ML 엔지니어, MLOps, 프라이버시 엔지니어, 데이터 디스커버리 서비스

## 데이터 웨어하우스와 데이터 레이크와 ETL/ELT
### 데이터 웨어하우스 옵션별 장단점
- 클라우드 기반 데이터 웨어하우스
- 고정비용 옵션과 가변비용 옵션

### 데이터 레이크
- 구조화 데이터와 비구조화 데이터 저장
- 클라우드 스토리지 사용 (AWS S3 등)

## ETL/ELT

### ETL (Extract, Transform, Load)
ETL는 데이터를 추출(Extract), 변환(Transform), 적재(Load)하는 과정을 말하며, 데이터 웨어하우스나 데이터 레이크로 데이터를 이동시키는 데 사용됩니다. ETL 프로세스는 다음과 같은 단계를 포함합니다:

1. **Extract (추출)**
   - **소스 시스템**으로부터 데이터를 추출합니다. 소스 시스템에는 관계형 데이터베이스, NoSQL 데이터베이스, 파일 시스템, API 등이 포함될 수 있습니다.
   - 데이터 추출 방법에는 완전 추출(Full Extraction)과 증분 추출(Incremental Extraction)이 있습니다.
     - **완전 추출**: 소스 시스템의 모든 데이터를 추출합니다. 주로 초기 로드나 데이터 복구 시 사용됩니다.
     - **증분 추출**: 변경된 데이터만 추출합니다. 운영 부담이 적고 실시간 데이터 처리에 유리합니다.

2. **Transform (변환)**
   - 추출한 데이터를 목적 시스템의 요구에 맞게 변환합니다. 변환 과정에는 데이터 정제(Cleaning), 데이터 통합(Integration), 데이터 집계(Aggregation), 데이터 정렬(Sorting), 데이터 계산(Calculation) 등이 포함됩니다.
   - **데이터 정제**: 결측값 처리, 중복 제거, 오류 데이터 수정 등.
   - **데이터 통합**: 여러 소스에서 데이터를 결합하여 일관된 형식으로 변환.
   - **데이터 집계**: 데이터를 요약하여 저장. 예를 들어, 일별 매출 데이터를 월별로 집계.
   - **데이터 정렬 및 계산**: 데이터 정렬, 수식 적용 등을 통해 데이터를 분석 가능한 상태로 변환.

3. **Load (적재)**
   - 변환된 데이터를 데이터 웨어하우스, 데이터 마트, 데이터 레이크 등에 적재합니다.
   - 적재 방식에는 전체 덮어쓰기(Bulk Load)와 증분 적재(Incremental Load)가 있습니다.
     - **전체 덮어쓰기**: 기존 데이터를 모두 삭제하고 새로운 데이터를 적재합니다. 대량 데이터 적재 시 유리하지만, 시간이 오래 걸립니다.
     - **증분 적재**: 변경된 데이터만 적재합니다. 실시간 데이터 처리와 지속적인 데이터 업데이트에 유리합니다.

### ELT (Extract, Load, Transform)
ELT는 데이터를 추출(Extract)하고, 적재(Load)한 후에 변환(Transform)하는 과정을 말합니다. ETL과 달리 데이터 웨어하우스 내에서 변환 작업을 수행하므로 대규모 데이터 처리에 유리합니다.

1. **Extract (추출)**
   - 소스 시스템으로부터 데이터를 추출하는 과정은 ETL과 동일합니다. 다양한 데이터 소스로부터 데이터를 추출하여 데이터 웨어하우스로 이동시킵니다.

2. **Load (적재)**
   - 추출한 데이터를 변환 없이 그대로 데이터 웨어하우스에 적재합니다. 이로 인해 초기 데이터 적재 속도가 빠릅니다.
   - 데이터 웨어하우스는 원시 데이터를 그대로 저장하여 필요할 때 변환할 수 있습니다.

3. **Transform (변환)**
   - 데이터 웨어하우스 내에서 변환 작업을 수행합니다. 데이터베이스 엔진의 고성능 처리 능력을 활용하여 변환 작업을 빠르게 수행할 수 있습니다.
   - ELT는 주로 대규모 데이터 세트를 처리할 때 사용되며, 데이터 마트, 분석용 데이터 셋 등의 생성에 유리합니다.

### 주요 ETL/ELT 도구와 프레임워크

1. **Apache Airflow**
   - **오픈소스 워크플로우 관리 플랫폼**으로, 복잡한 ETL 파이프라인을 정의하고 스케줄링할 수 있습니다.
   - DAG(Directed Acyclic Graph)를 사용하여 작업 간의 의존성을 관리합니다.
   - 파이썬 기반으로 유연한 ETL 파이프라인 구성이 가능합니다.
   - 주요 기능: 태스크 스케줄링, 모니터링, 로깅, 에러 핸들링.

2. **Apache NiFi**
   - **데이터 흐름 관리 도구**로, 실시간 데이터 스트리밍과 배치 처리에 모두 사용됩니다.
   - GUI 기반으로 데이터 플로우를 정의할 수 있어 사용자 친화적입니다.
   - 주요 기능: 데이터 소스와 싱크 간의 데이터 이동, 데이터 변환, 실시간 모니터링.

3. **dbt (data build tool)**
   - **ELT 작업을 위한 오픈소스 도구**로, SQL을 사용하여 데이터 변환을 정의합니다.
   - 데이터 변환 작업을 모듈화하고 재사용 가능한 방식으로 관리할 수 있습니다.
   - 주요 기능: 모델링, 테스트, 문서화, 변환 파이프라인 자동화.

4. **Talend**
   - **엔터프라이즈 데이터 통합 도구**로, ETL 작업을 시각적으로 구성할 수 있습니다.
   - 데이터 품질 관리, 메타데이터 관리, 빅데이터 통합 기능을 제공합니다.
   - 주요 기능: ETL 작업 자동화, 데이터 품질 검사, 실시간 데이터 통합.

5. **Informatica PowerCenter**
   - **데이터 통합 플랫폼**으로, 복잡한 ETL 파이프라인을 효율적으로 관리할 수 있습니다.
   - 대규모 데이터 처리와 고성능을 요구하는 환경에 적합합니다.
   - 주요 기능: 데이터 추출, 변환, 적재, 데이터 품질 관리, 메타데이터 관리.

### ETL/ELT 베스트 프랙티스

1. **데이터 품질 보장**
   - 데이터 추출 단계에서 데이터 품질 검사를 수행하여 잘못된 데이터를 걸러냅니다.
   - 변환 단계에서 데이터 정제를 통해 일관된 데이터를 유지합니다.

2. **변경 데이터 캡처 (CDC, Change Data Capture)**
   - 소스 시스템의 변경 사항만 추출하여 데이터 처리량을 줄이고, 실시간 데이터 동기화를 유지합니다.

3. **데이터 보안**
   - ETL/ELT 파이프라인 전반에 걸쳐 데이터 암호화, 접근 제어, 감사 로깅을 적용합니다.
   - 민감한 데이터의 경우 데이터 마스킹을 사용하여 보안을 강화합니다.

4. **모니터링과 알림**
   - ETL/ELT 작업의 상태를 모니터링하고, 오류 발생 시 즉시 알림을 설정하여 문제를 신속하게 해결합니다.
   - 로그와 메트릭을 수집하여 성능을 분석하고 최적화합니다.

5. **성능 최적화**
   - ETL/ELT 작업의 병렬 처리와 분산 처리를 통해 처리 속도를 향상시킵니다.
   - 인덱스, 파티셔닝, 클러스터링 등을 사용하여 데이터베이스 성능을 최적화합니다.

6. **문서화와 버전 관리**
   - ETL/ELT 파이프라인과 변환 로직을 철저히 문서화하여 유지보수성을 높입니다.
   - 버전 관리 시스템을 사용하여 ETL/ELT 코드와 설정의 변경 이력을 추적합니다.

### ETL/ELT의 최신 트렌드

1. **데이터 오케스트레이션**
   - 데이터 오케스트레이션 도구를 사용하여 다양한 데이터 파이프라인과 워크플로우를 통합 관리합니다.
   - 예: Apache Airflow, Prefect, Dagster.

2. **클라우드 기반 ETL/ELT**
   - 클라우드 네이티브 ETL/ELT 도구를 사용하여 유연성과 확장성을 확보합니다.
   - 예: AWS Glue, Google Cloud Dataflow, Azure Data Factory.

3. **실시간 데이터 처리**
   - 스트리밍 데이터를 실시간으로 처리하여 신속한 의사 결정을 지원합니다.
   - 예: Apache Kafka, Apache Flink, Amazon Kinesis.

4. **자동화와 머신러닝**
   - ETL/ELT 작업에 머신러닝을 도입하여 데이터 정제, 변환, 적재 과정을 자동화합니다.
   - 예: 데이터 변환 규칙 학습, 데이터 품질 검사 자동화.

5. **데이터 파이프라인의 셀프서비스**
   - 비즈니스 사용자가 직접 ETL/ELT 파이프라인을 구성하고 관리할 수 있는 셀프서비스 도구 제공.
   - 예: Alteryx, Dataiku, Trifacta.

![ETL Process](/assets/images/ETL_process.png)

## 결론
ETL과 ELT는 데이터 통합과 분석의 핵심 요소입니다. 각 기법의 장단점을 이해하고, 적절한 도구와 베스트 프랙티스를 적용함으로써 효율적인 데이터 파이프라인을 구축할 수 있습니다. 최신 트렌드와 기술을 활용하여 데이터 처리의 효율성과 신뢰성을 높이는 것이 중요합니다.


## 빅데이터 처리 프레임워크
- 분산 환경 기반의 데이터 처리 시스템 (Hadoop, Spark)
- 대표적 빅데이터 프로세싱 시스템: Mapreduce, Hive, Presto, Spark

## 데이터 웨어하우스 옵션들
- AWS Redshift, Snowflake, Google Cloud BigQuery, Apache Hive, Apache Presto, Apache Iceberg, Apache Spark

## 실리콘밸리 회사들의 데이터 스택 트렌드
- 초기 단계: 데이터 웨어하우스 + ETL
- 발전 단계: 데이터 양 증가와 Spark 도입
- 성숙 단계: 데이터 활용 증대 및 MLOps 도입

### 실리콘밸리 회사들의 데이터 스택 비교
- 예: Apple, Affirm, Pinterest, Zillow, PepsiCo, Confluent, Udemy, Uber, Airbnb
